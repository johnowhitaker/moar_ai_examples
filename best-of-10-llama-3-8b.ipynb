{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qq alpaca-eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Best-of-10 Sampling\n",
    "\n",
    "This noteook shows how you can get a boost in performance (as measured by Alpacaeval) by sampling multiple times and choosing the best result.\n",
    "\n",
    "I ran it on my dual 3090 machine, with the reward model on one GPU and the LLM on the other. \n",
    "\n",
    "Since this ran over the weekend I didn't optimize it at all - you'll likely want to generate or score in batches and look into optimizations for inference if you want to replicate this in less then 24 hours!\n",
    "\n",
    "I've edited it lightly to add comments and run it on only 3 samples by default.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a reward model, which assigns a score to a chat interaction. I picked the best open one on [RewardBench](https://huggingface.co/spaces/allenai/reward-bench) and copied in the usage example here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8db6402d0d648869c7cb13a9759edca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the reward model\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained(\"sfairXC/FsfairX-LLaMA3-RM-v0.1\")\n",
    "rm_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"sfairXC/FsfairX-LLaMA3-RM-v0.1\",\n",
    "    device=0,\n",
    "    tokenizer=rm_tokenizer,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
    ")\n",
    "\n",
    "pipe_kwargs = {\n",
    "    \"return_all_scores\": True,\n",
    "    \"function_to_apply\": \"none\",\n",
    "    \"batch_size\": 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's score a sample conversation. Try changing the assistant response to something nasty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.53125\n"
     ]
    }
   ],
   "source": [
    "# Test it (usage example from HF page)\n",
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"I'm doing well! How can I help?\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "]\n",
    "\n",
    "test_texts = [rm_tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False).replace(rm_tokenizer.bos_token, \"\")]\n",
    "pipe_outputs = rm_pipe(test_texts, **pipe_kwargs)\n",
    "rewards = [output[0][\"score\"] for output in pipe_outputs]\n",
    "print(rewards[0]) # -8.1 if we add a naughty word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Alpacaeval Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I copied the starter code from the Llama 3 8B Instruct page to generate some text in response to an instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58cc0e0adfd24c66862a7797e6ebfb8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb24686868c1406495ed0325bf145ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf87a63f6736430dbc5b246139d0cba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrr, me hearty! Me name be Captain Chatbot, the scurviest pirate to ever sail the Seven Seas! Me be a chatbot, aye, but me be a pirate at heart, with a penchant fer swashbucklin' and a love o' treasure! Me be here to chat with ye, share me tales o' adventure, and maybe even help ye find yer own treasure! So hoist the colors, me hearty, and let's set sail fer a swashbucklin' good time!\n"
     ]
    }
   ],
   "source": [
    "# Load the model and generate some text\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=1\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package that up into a function that takes an instruction and returns a response\n",
    "def get_response(instruction):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "    ]\n",
    "\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][len(prompt):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to use this to generate responses to the instructions used in AlpacaEval 2.0, a popular evaluation of chat models. Let's see what a submission is supposed to look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the alpacaeval instructions\n",
    "import requests\n",
    "\n",
    "example_json_url = \"https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/example/outputs.json\"\n",
    "\n",
    "response = requests.get(example_json_url)\n",
    "examples = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's one example (they inclide output from a simple model which we'll replace):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'helpful_base',\n",
      " 'datasplit': 'eval',\n",
      " 'generator': 'example',\n",
      " 'instruction': 'What are the names of some famous actors that started their '\n",
      "                'careers on Broadway?',\n",
      " 'output': 'Some famous actors that started their careers on Broadway are Hugh '\n",
      "           'Jackman, Meryl Streep, Denzel Washington, Audra McDonald, and '\n",
      "           'Lin-Manuel Miranda.'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get our own model's response to this instruction like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many talented actors have gotten their start on Broadway before making it big in Hollywood. Here are some famous actors who began their careers on the Great White Way:\n",
      "\n",
      "1. Hugh Jackman - Jackman originated the role of Curly McLain in the 1998 Broadway production of \"Oklahoma!\" before moving on to star in films like the X-Men franchise and \"Les Misérables.\"\n",
      "2. Neil Patrick Harris - Harris got his start on Broadway in the 1980s, appearing in productions like \"Assassins\" and \"Cabaret.\" He's since become a household name thanks to his roles in TV shows like \"How I Met Your Mother\" and \"Doogie Howser, M.D.\"\n",
      "3. Scarlett Johansson - Johansson made her Broadway debut in the 1995 production of \"A View from the Bridge\" alongside Robert De Niro. She's since become a Hollywood A-lister, starring in films like the Marvel Cinematic Universe and \"Lost in Translation.\"\n",
      "4. Chris Evans - Evans originated the role of Billy in the 2000 Broadway production of \"Miss Saigon\" before moving on to star in the Marvel Cinematic Universe as Captain America.\n",
      "5. Anne Hathaway - Hathaway made her Broadway debut in the 2003 production of \"Jane Eyre\" before winning an Oscar for her role in \"Les Misérables\" and starring in films like \"The Devil Wears Prada\" and \"Ocean's 8.\"\n",
      "6. Jake Gyllenhaal - Gyllenhaal got his start on Broadway in the 2002 production of \"This Is Our Youth,\" and has since appeared in films like \"Brokeback Mountain,\" \"Nightcrawler,\" and \"Prisoners.\"\n",
      "7. Idina Menzel - Menzel originated the role of Maureen Johnson in the 1995 Broadway production of \"Rent\" and went on to star in films like \"Enchanted\" and \"Frozen,\" for which she won an Academy Award for Best Original Song.\n",
      "8. Lin-Manuel Miranda - Miranda created the hit Broadway musicals \"In the Heights\" and \"Hamilton,\" and has also appeared in films like \"Mary Poppins Returns\" and \"Star Wars: The Rise of Skywalker.\"\n",
      "9. Zendaya - Zendaya made her Broadway debut in the 2013 production of \"Spider-Man: Turn Off the Dark\" before moving on to star in TV shows like \"Euphoria\" and films like \"Spider-Man: Homecoming.\"\n",
      "10. Josh Gad - Gad originated the role of Elder Arnold Cunningham in the 2011 Broadway production of \"The Book of Mormon\" and has since appeared in films like \"Frozen,\" \"Beauty and the Beast,\" and \"Murder on the Orient Express.\"\n",
      "\n",
      "These are just a few examples of the many talented actors who got their start on Broadway before making it big in Hollywood.\n"
     ]
    }
   ],
   "source": [
    "print(get_response(examples[0]['instruction']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this for all examples, generating 10 responses and storing the first plus the best (according to the reward model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through all examples, generating and scoring responses\n",
    "first_outputs = []\n",
    "chosen_outputs = []\n",
    "n_tries = 10\n",
    "for e in tqdm(examples[:3]):\n",
    "    instruction = e['instruction']\n",
    "    responses, scores = [], []\n",
    "    for i in range(n_tries):\n",
    "        response = get_response(instruction)\n",
    "        chat = [\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "        {\"role\": \"assistant\", \"content\": response},\n",
    "        ]\n",
    "\n",
    "        test_texts = [rm_tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False).replace(rm_tokenizer.bos_token, \"\")]\n",
    "        pipe_outputs = rm_pipe(test_texts, **pipe_kwargs)\n",
    "        rewards = [output[0][\"score\"] for output in pipe_outputs]\n",
    "        responses.append(response)\n",
    "        scores.append(rewards[0]) \n",
    "\n",
    "    # Store the first response and the best response\n",
    "    first_outputs.append({\n",
    "        'dataset': e['dataset'],\n",
    "        'datasplit': e['dataset'],\n",
    "        'generator': 'l3_8b_default',\n",
    "        'instruction': instruction,\n",
    "        'output': responses[0]\n",
    "    })\n",
    "    chosen_outputs.append({\n",
    "        'dataset': e['dataset'],\n",
    "        'datasplit': e['dataset'],\n",
    "        'generator': f'l3_8b_best_of_{n_tries}',\n",
    "        'instruction': instruction,\n",
    "        'output': responses[np.argmax(np.array(scores))] # << The best of the n_tries responses\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs for evaluation\n",
    "import json\n",
    "with open('first_outputs.json', 'w') as f:\n",
    "    f.write(json.dumps(first_outputs))\n",
    "with open('chosen_outputs.json', 'w') as f:\n",
    "    f.write(json.dumps(chosen_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run eval:\n",
    "```\n",
    "export OPENAI_API_KEY=<your_api_key>\n",
    "alpaca_eval --model_outputs 'first_outputs.json' \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlpacaEval takes these responses and compares them to those written by GPT4-Turbo, with a big LLM (GPT-4 by default) as the judge picking which response is better. The output is a 'win rate' score - the higher the better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
